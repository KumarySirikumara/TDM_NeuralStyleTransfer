{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ML_IT17168250",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ee2dfa402154761be5daef4944f1e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2dd2ade5748849b082320feaa6b2e6f4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_23ec04e9d5394740b4b5b255c957125d",
              "IPY_MODEL_01b41310293a4dec90cc2e15eb066a6e"
            ]
          }
        },
        "2dd2ade5748849b082320feaa6b2e6f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23ec04e9d5394740b4b5b255c957125d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8e47897d66994ac89c6804fbed955b90",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 25020748,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25020748,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d03aacdd696948fe9a6562ef2a92b470"
          }
        },
        "01b41310293a4dec90cc2e15eb066a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b32bb054536d4636ac24ab0788827e94",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23.9M/23.9M [00:55&lt;00:00, 449kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b3aa4ada22e94273a6cfa4c6a708b2f5"
          }
        },
        "8e47897d66994ac89c6804fbed955b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d03aacdd696948fe9a6562ef2a92b470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b32bb054536d4636ac24ab0788827e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b3aa4ada22e94273a6cfa4c6a708b2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2dddad879eb34bb68fa0c657c48fcf16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2ae94ebff36347e9b9f6fb85869acf8a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0ca1294ddfc945d3b014ed50e80e5313",
              "IPY_MODEL_75866285aaae4e1fab3529bb7871ece8"
            ]
          }
        },
        "2ae94ebff36347e9b9f6fb85869acf8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ca1294ddfc945d3b014ed50e80e5313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_134790a53a2d40cc9a1428cb7883627a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 25020748,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25020748,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e434c57cb0174548a633844e4ba3a645"
          }
        },
        "75866285aaae4e1fab3529bb7871ece8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a9978d53ff824aba86ea5cfaa983266b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23.9M/23.9M [00:03&lt;00:00, 6.96MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a28fb226d26d47cabbcae4bdca87478c"
          }
        },
        "134790a53a2d40cc9a1428cb7883627a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e434c57cb0174548a633844e4ba3a645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9978d53ff824aba86ea5cfaa983266b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a28fb226d26d47cabbcae4bdca87478c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWkEh1wXLiul"
      },
      "source": [
        "## ML Assignment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLmP_yWoQ665",
        "outputId": "158351f8-408d-4988-95aa-70156aaa5800"
      },
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-xynull6q\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-xynull6q\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (2.0.2)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (1.1.0)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (0.8.9)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (4.41.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (2.5.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/69/805702ba4c2ae87ab887605619f0478e10ad0aa92efd67c28e0761c72e6b/fvcore-0.1.5.post20210624.tar.gz (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hCollecting iopath<0.1.9,>=0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/21/d0/22104caed16fa41382702fed959f4a9b088b2f905e7a82e4483180a2ec2a/iopath-0.1.8-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.4.1) (1.3.0)\n",
            "Collecting omegaconf>=2.1.0rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/96/1966b48bfe6ca64bfadfa7bcc9a8d73c5d83b4be769321fcc5d617abeb0c/omegaconf-2.1.0-py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.2MB/s \n",
            "\u001b[?25hCollecting hydra-core>=1.1.0rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/cd/85aa2e3a8babc36feac99df785e54abf99afbc4acc20488630f3ef46980a/hydra_core-1.1.0-py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 44.0MB/s \n",
            "\u001b[?25hCollecting black==21.4b2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/c3/848edbd902fa908e941eaf72dc142b4a5c86e903c1e0129cf7cd098a485b/black-21.4b2-py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4.1) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4.1) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4.1) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4.1) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.4.1) (2.4.7)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.4.1) (57.0.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.4.1) (0.29.23)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from yacs>=0.1.6->detectron2==0.4.1) (3.13)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (3.3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (1.34.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.4.1) (1.31.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1.0rc1->detectron2==0.4.1) (5.1.4)\n",
            "Collecting regex>=2020.1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/28/5f08d8841013ccf72cd95dfff2500fe7fb39467af12c5e7b802d8381d811/regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720kB)\n",
            "\u001b[K     |████████████████████████████████| 727kB 46.5MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.4.1) (3.7.4.3)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.4.1) (0.10.2)\n",
            "Collecting typed-ast>=1.4.2; python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.4.1) (7.1.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.4.1) (1.4.4)\n",
            "Collecting pathspec<1,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/29/29/a465741a3d97ea3c17d21eaad4c64205428bde56742360876c4391f930d4/pathspec-0.8.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->detectron2==0.4.1) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.4.1) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4.1) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4.1) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.4.1) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.4.1) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.4.1) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.4.1) (0.2.8)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core>=1.1.0rc1->detectron2==0.4.1) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.4.1) (3.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard->detectron2==0.4.1) (0.4.8)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.4.1-cp37-cp37m-linux_x86_64.whl size=5515659 sha256=7102fb41ab30afb67f6f7d04a9226d7bfb303d2bed4c3aa93b18f726f94973a8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-aom29sle/wheels/33/ac/bb/5ef90585c21c67e2f0b6aae55ec6b43017ad57af33d5f4c339\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20210624-cp37-none-any.whl size=60611 sha256=5cb9782abd894aac6c94c14f96846e6bd8471c7ffdfc4b187a281a8797385770\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/3d/8b/911fda7dc70c40197710f00c0e0b6d0d11c4d0bfb620567894\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=8b3084f356e9f64e5bd57be5fc4eaad5980861b7a9ba29299a4d7e33c1abddcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "\u001b[31mERROR: fvcore 0.1.5.post20210624 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: omegaconf 2.1.0 has requirement PyYAML>=5.1.*, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore, antlr4-python3-runtime, omegaconf, hydra-core, regex, mypy-extensions, typed-ast, pathspec, black, detectron2\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed antlr4-python3-runtime-4.8 black-21.4b2 detectron2-0.4.1 fvcore-0.1.5.post20210624 hydra-core-1.1.0 iopath-0.1.8 mypy-extensions-0.4.3 omegaconf-2.1.0 pathspec-0.8.1 portalocker-2.3.0 regex-2021.4.4 typed-ast-1.4.3 yacs-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQBEw61pUjPN",
        "cellView": "form"
      },
      "source": [
        "#@title Select options\n",
        "\n",
        "import os\n",
        "import sys\n",
        "PEOPLE = False #@param {type:\"boolean\"}\n",
        "SKATEBOARD = False #@param {type:\"boolean\"}\n",
        "OTHER_CLASSES = 'car' #@param {type:\"string\"}\n",
        "STYLE = \"mosaic\" #@param [\"candy\", \"mosaic\", \"rain_princess\", \"udnie\", \"pretrained\"]\n",
        "\n",
        "INVERSE = False #@param {type:\"boolean\"}\n",
        "CLASS_SPECIFIC = True #@param {type:\"boolean\"}\n",
        "FPS_30 = False #@param {type:\"boolean\"}\n",
        "\n",
        "STYLE_PTH = \"saved_models/\" + STYLE + \".pth\"\n",
        "\n",
        "\n",
        "objects = []\n",
        "if PEOPLE:\n",
        "  objects.append(\"person\")\n",
        "if SKATEBOARD:\n",
        "  objects.append(\"skateboard\")\n",
        "if OTHER_CLASSES:\n",
        "  other_list = OTHER_CLASSES.split(\",\")\n",
        "  for ob in other_list:\n",
        "    objects.append(ob.lower().strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "9zeJRA08Wiow",
        "outputId": "1804dae2-0b3a-4fdc-9aca-1ee14f264236"
      },
      "source": [
        "#@title Select file(s)\n",
        "\n",
        "from google.colab import files\n",
        "print(\"Upload media\")\n",
        "files_list = files.upload()\n",
        "\n",
        "import sys\n",
        "!pip install filetype\n",
        "import filetype\n",
        "\n",
        "# Select media file\n",
        "media_formats = [\"video\", \"image\"]\n",
        "for key in files_list:\n",
        "  kind = filetype.guess(key)\n",
        "  file_type = kind.mime.split(\"/\")[0]\n",
        "  if file_type not in media_formats:\n",
        "    print(\"Unrecognized media file: \" + key)\n",
        "    sys.exit()\n",
        "  print(kind.mime)\n",
        "  print(key)\n",
        "  file_name = key\n",
        "\n",
        "# Select pretrained model weights\n",
        "if STYLE == \"pretrained\":\n",
        "  print(\"Upload pretrained weights\")\n",
        "  files_list = files.upload()\n",
        "  for key in files_list:\n",
        "    ext = key.split(\".\")[:1]\n",
        "    if ext.lower != \"pth\":\n",
        "      print(\"Not valid model weight: \" + key)\n",
        "      sys.exit()\n",
        "    else:\n",
        "      STYLE_PTH = KEY"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Upload media\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-261a7076-5aad-4227-9779-100453f2302b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-261a7076-5aad-4227-9779-100453f2302b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving background.jpg to background.jpg\n",
            "Saving car race.jpg to car race.jpg\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.7/dist-packages (1.0.7)\n",
            "image/jpeg\n",
            "background.jpg\n",
            "image/jpeg\n",
            "car race.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb7aOmDcfZCu"
      },
      "source": [
        "# import some common \n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import subprocess\n",
        "\n",
        "import cv2\n",
        "import filetype\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmweh5lkfvHL",
        "outputId": "9d1d74e1-ec5c-4ae5-96d7-32d6d92397fd"
      },
      "source": [
        "!pip install pyyaml==5.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyyaml==5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 17.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 15.2MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 7.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44091 sha256=ca7398c64f9f26d968c0cd80f07b96aebd6ae99717a2af62a8257b27c656b640\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbkMmyvxfzHc"
      },
      "source": [
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.7)\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "import torch\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByTl-QGPmhdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186,
          "referenced_widgets": [
            "5ee2dfa402154761be5daef4944f1e1c",
            "2dd2ade5748849b082320feaa6b2e6f4",
            "23ec04e9d5394740b4b5b255c957125d",
            "01b41310293a4dec90cc2e15eb066a6e",
            "8e47897d66994ac89c6804fbed955b90",
            "d03aacdd696948fe9a6562ef2a92b470",
            "b32bb054536d4636ac24ab0788827e94",
            "b3aa4ada22e94273a6cfa4c6a708b2f5"
          ]
        },
        "outputId": "f83d6ff7-dc91-43f1-d1d4-3d08da238371"
      },
      "source": [
        "#@title Collecting stuff...\n",
        "# import some common \n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import subprocess\n",
        "\n",
        "import cv2\n",
        "import filetype\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "!pip install pyyaml==5.1\n",
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.7)\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "import torch\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html &> /dev/null\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "!pip install ffmpeg-python\n",
        "import ffmpeg\n",
        "# Installing pretrained model weights\n",
        "import zipfile\n",
        "\n",
        "def unzip(source_filename, dest_dir):\n",
        "    with zipfile.ZipFile(source_filename) as zf:\n",
        "        zf.extractall(path=dest_dir)\n",
        "\n",
        "try:\n",
        "    from torch.utils.model_zoo import _download_url_to_file\n",
        "except ImportError:\n",
        "    try:\n",
        "        from torch.hub import download_url_to_file as _download_url_to_file\n",
        "    except ImportError:\n",
        "        from torch.hub import _download_url_to_file\n",
        "\n",
        "_download_url_to_file('https://www.dropbox.com/s/lrvwfehqdcxoza8/saved_models.zip?dl=1', 'saved_models.zip', None, True)\n",
        "unzip('saved_models.zip', '.')\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "media_folder = \"./media_\" + timestamp\n",
        "\n",
        "\n",
        "FINISHED_PRODUCTS = media_folder  + \"/\"\n",
        "# Creating directories\n",
        "command = \"mkdir \" + FINISHED_PRODUCTS\n",
        "subprocess.call(command, shell=True)\n",
        "\n",
        "TEMP_DIR = \"temp_imgs/\"\n",
        "ORG_DIR = \"temp_imgs/org_imgs/\"\n",
        "STYLE_DIR = \"temp_imgs/style_imgs/\"\n",
        "MASKS_DIR = \"temp_imgs/masks_imgs/\"\n",
        "FINAL_DIR = \"temp_imgs/final_imgs/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.7/dist-packages (5.1)\n",
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ee2dfa402154761be5daef4944f1e1c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=25020748.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-5oGO6_hpXR",
        "outputId": "c6ec03e5-3f3f-4092-e8cf-33df58e9f8df"
      },
      "source": [
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html &> /dev/null\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Logger detectron2 (DEBUG)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTc0QSyfhtWZ"
      },
      "source": [
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMILfs-xhvyX",
        "outputId": "a30b1a93-1913-4368-b5d0-f2ecc45b07ce"
      },
      "source": [
        "!pip install ffmpeg-python\n",
        "import ffmpeg\n",
        "# Installing pretrained model weights\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "2dddad879eb34bb68fa0c657c48fcf16",
            "2ae94ebff36347e9b9f6fb85869acf8a",
            "0ca1294ddfc945d3b014ed50e80e5313",
            "75866285aaae4e1fab3529bb7871ece8",
            "134790a53a2d40cc9a1428cb7883627a",
            "e434c57cb0174548a633844e4ba3a645",
            "a9978d53ff824aba86ea5cfaa983266b",
            "a28fb226d26d47cabbcae4bdca87478c"
          ]
        },
        "id": "l4nhETfVh2WX",
        "outputId": "53b15e47-4e52-45f5-cfe6-dc8a6df1ac98"
      },
      "source": [
        "def unzip(source_filename, dest_dir):\n",
        "    with zipfile.ZipFile(source_filename) as zf:\n",
        "        zf.extractall(path=dest_dir)\n",
        "\n",
        "try:\n",
        "    from torch.utils.model_zoo import _download_url_to_file\n",
        "except ImportError:\n",
        "    try:\n",
        "        from torch.hub import download_url_to_file as _download_url_to_file\n",
        "    except ImportError:\n",
        "        from torch.hub import _download_url_to_file\n",
        "\n",
        "_download_url_to_file('https://www.dropbox.com/s/lrvwfehqdcxoza8/saved_models.zip?dl=1', 'saved_models.zip', None, True)\n",
        "unzip('saved_models.zip', '.')\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "media_folder = \"./media_\" + timestamp\n",
        "\n",
        "\n",
        "FINISHED_PRODUCTS = media_folder  + \"/\"\n",
        "# Creating directories\n",
        "command = \"mkdir \" + FINISHED_PRODUCTS\n",
        "subprocess.call(command, shell=True)\n",
        "\n",
        "TEMP_DIR = \"temp_imgs/\"\n",
        "ORG_DIR = \"temp_imgs/org_imgs/\"\n",
        "STYLE_DIR = \"temp_imgs/style_imgs/\"\n",
        "MASKS_DIR = \"temp_imgs/masks_imgs/\"\n",
        "FINAL_DIR = \"temp_imgs/final_imgs/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dddad879eb34bb68fa0c657c48fcf16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=25020748.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp-SPX1ShoQE"
      },
      "source": [
        "#@title Utilities\n",
        "\n",
        "def create_mask(outputs, num_class_list, img):\n",
        "  '''Creates binary masks of detected objects'''\n",
        "  class_pred = outputs[\"instances\"].pred_classes.detach().cpu().numpy()\n",
        "  masks_pred = outputs[\"instances\"].pred_masks.detach().cpu().numpy()\n",
        "  bin_mask = np.zeros(img.shape[:2])\n",
        "  if len(class_pred) != 0:\n",
        "    for cnt, c in enumerate(class_pred):\n",
        "      if c in num_class_list:\n",
        "        bin_mask += masks_pred[cnt]*1\n",
        "  bin_mask[bin_mask > 0] = 1\n",
        "  return bin_mask\n",
        "\n",
        "def check_rotation(path_video_file):\n",
        "  # this returns meta-data of the video file in form of a dictionary\n",
        "  meta_dict = ffmpeg.probe(path_video_file)\n",
        "\n",
        "  # from the dictionary, meta_dict['streams'][0]['tags']['rotate'] is the key\n",
        "  # we are looking for\n",
        "  rotateCode = None\n",
        "  try:\n",
        "    if int(meta_dict['streams'][0]['tags']['rotate']) == 90:\n",
        "        rotateCode = cv2.ROTATE_90_CLOCKWISE\n",
        "    elif int(meta_dict['streams'][0]['tags']['rotate']) == 180:\n",
        "        rotateCode = cv2.ROTATE_180\n",
        "    elif int(meta_dict['streams'][0]['tags']['rotate']) == 270:\n",
        "        rotateCode = cv2.ROTATE_90_COUNTERCLOCKWISE\n",
        "    print(\"Rotation\")\n",
        "  except KeyError:\n",
        "    print(\"No rotation\")\n",
        "  return rotateCode\n",
        "\n",
        "def correct_rotation(frame, rotateCode):  \n",
        "  return cv2.rotate(frame, rotateCode) \n",
        "\n",
        "def extract_frames(file_name):\n",
        "  '''Extract frames from video \"file_name\" '''\n",
        "  vidcap = cv2.VideoCapture(file_name)\n",
        "  rotate_code = check_rotation(file_name)\n",
        "\n",
        "  fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "  if FPS_30:\n",
        "    factor_fps = round(fps/30)\n",
        "\n",
        "  success,image = vidcap.read()\n",
        "  frame_num = 0\n",
        "  skip_num = 0\n",
        "  img_list = []\n",
        "  base_names = []\n",
        "  while success:\n",
        "    if rotate_code is not None:\n",
        "         image = correct_rotation(image, rotate_code)\n",
        "    if FPS_30: \n",
        "      if not skip_num%factor_fps:\n",
        "        cv2.imwrite(ORG_DIR + \"frame_{0:05}.png\".format(frame_num), image)\n",
        "        base_names.append(\"frame_{0:05}.png\".format(frame_num))       \n",
        "        frame_num += 1 \n",
        "      skip_num += 1\n",
        "    else:\n",
        "      cv2.imwrite(ORG_DIR + \"frame_{0:05}.png\".format(frame_num), image)\n",
        "      base_names.append(\"frame_{0:05}.png\".format(frame_num))       \n",
        "      frame_num += 1 \n",
        "    success,image = vidcap.read()\n",
        "  base_names.sort()\n",
        "  return base_names, vidcap\n",
        "\n",
        "def final_imgs(base_names):\n",
        "  '''Combines the style and mask images to create final frames.'''\n",
        "  for base in tqdm(base_names):\n",
        "      if CLASS_SPECIFIC:\n",
        "        frame_img = cv2.imread(ORG_DIR + base)\n",
        "        style_img = cv2.imread(STYLE_DIR + base)\n",
        "        mask_img = cv2.imread(MASKS_DIR + base)\n",
        "\n",
        "        if style_img.shape != frame_img.shape:\n",
        "          f_w, f_h, c = frame_img.shape\n",
        "          style_img = cv2.resize(style_img, (f_h, f_w), interpolation = cv2.INTER_AREA)\n",
        "        if INVERSE:\n",
        "          frame_img[mask_img==0] = 0\n",
        "          style_img[mask_img==1] = 0\n",
        "        else:\n",
        "          frame_img[mask_img==1] = 0\n",
        "          style_img[mask_img==0] = 0\n",
        "        final_img = frame_img + style_img\n",
        "        cv2.imwrite(FINAL_DIR + base, final_img)\n",
        "      else:\n",
        "        style_img = cv2.imread(STYLE_DIR + base)\n",
        "        cv2.imwrite(FINAL_DIR + base, style_img)\n",
        "  return\n",
        "\n",
        "def create_vid(base_names, vidcap, save_name):\n",
        "  '''Creates video from final frames'''\n",
        "  img = cv2.imread(ORG_DIR + base_names[0])\n",
        "  height, width, layers = img.shape\n",
        "  size = (width,height)\n",
        "  if FPS_30:\n",
        "    fps = 30\n",
        "  else:\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "  print(fps)\n",
        "  \n",
        "  fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
        "  out = cv2.VideoWriter(TEMP_DIR + 'vid_nosound.mp4',  #Provide a file to write the video to\n",
        "                        fourcc,\n",
        "                        round(fps),                                        \n",
        "                        size)\n",
        "  for base in tqdm(base_names):\n",
        "    img = cv2.imread(FINAL_DIR + base)\n",
        "    out.write(img)\n",
        "  out.release()\n",
        "  #command = \"ffmpeg -i temp_imgs/final_imgs/frame_%05d.png -c:v libx264 -vf fps=\" + str(round(fps)) + \" \" + TEMP_DIR + 'vid_nosound.mp4'\n",
        "  #subprocess.call(command, shell=True)\n",
        "  \n",
        "  command = \"ffmpeg -i \" + file_name + \" -ab 160k -ac 2 -ar 44100 -vn \" + TEMP_DIR + \"audio.wav\"\n",
        "  subprocess.call(command, shell=True)\n",
        "\n",
        "  command = \"ffmpeg -i \" + TEMP_DIR + 'vid_nosound.mp4' + \" -i \" + TEMP_DIR + \"audio.wav -c:v copy -c:a aac \" + TEMP_DIR + \"vid_b_conv.mp4\"\n",
        "  subprocess.call(command, shell=True)\n",
        "\n",
        "  if save_name is None:\n",
        "    save_name = file_name + \"style\"\n",
        "  command = \"ffmpeg -i \" + TEMP_DIR + \"vid_b_conv.mp4 -vcodec libx264 -profile:v main -level 3.1 -preset medium -crf 23 -x264-params ref=4 -acodec copy -movflags +faststart  \" + FINISHED_PRODUCTS + save_name + \".mp4\"\n",
        "  subprocess.call(command, shell=True)\n",
        "  return\n",
        "\n",
        "def remove_temp():\n",
        "  ! rm -r temp_imgs\n",
        "  return\n",
        "\n",
        "def create_temp():\n",
        "  !mkdir -p temp_imgs\n",
        "  !mkdir -p temp_imgs/org_imgs/\n",
        "  !mkdir -p temp_imgs/style_imgs/\n",
        "  !mkdir -p temp_imgs/masks_imgs/\n",
        "  !mkdir -p temp_imgs/final_imgs/\n",
        "  !mkdir -p temp_imgs/changed_background_imgs/\n",
        "  return\n",
        "\n",
        "def create_name(key):\n",
        "  base = key.split(\".\")[0] + \"_\" + STYLE\n",
        "  if INVERSE:\n",
        "    base = base + \"_INVERSE\"\n",
        "  if not CLASS_SPECIFIC:\n",
        "    base = base + \"_NOTSPECIFIC\"\n",
        "  for ob in objects:\n",
        "    base = base + \"_\" + ob\n",
        "  return base + \"_\" + timestamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QFSIG4UeCDS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8GLjKYehWsm"
      },
      "source": [
        "#@title Models\n",
        "class TransformerNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "        # Initial convolution layers\n",
        "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
        "        # Residual layers\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "        # Upsampling Layers\n",
        "        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "        # Non-linearities\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    \"\"\"ResidualBlock\n",
        "    introduced in: https://arxiv.org/abs/1512.03385\n",
        "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpsampleConvLayer(torch.nn.Module):\n",
        "    \"\"\"UpsampleConvLayer\n",
        "    Upsamples the input and then does a convolution. This method gives better results\n",
        "    compared to ConvTranspose2d.\n",
        "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiQZbhdYiF9s"
      },
      "source": [
        "class TransformerNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "        # Initial convolution layers\n",
        "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
        "        # Residual layers\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "        # Upsampling Layers\n",
        "        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "        # Non-linearities\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    \"\"\"ResidualBlock\n",
        "    introduced in: https://arxiv.org/abs/1512.03385\n",
        "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpsampleConvLayer(torch.nn.Module):\n",
        "    \"\"\"UpsampleConvLayer\n",
        "    Upsamples the input and then does a convolution. This method gives better results\n",
        "    compared to ConvTranspose2d.\n",
        "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvFQzOjJmspr"
      },
      "source": [
        "#@title Engine\n",
        "def run_engine(base_names):\n",
        "  # stylization setup\n",
        "  content_transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Lambda(lambda x: x.mul(255))\n",
        "  ])\n",
        "  with torch.no_grad():\n",
        "      style_model = TransformerNet()\n",
        "      state_dict = torch.load(STYLE_PTH)\n",
        "      # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
        "      for k in list(state_dict.keys()):\n",
        "          if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
        "              del state_dict[k]\n",
        "      style_model.load_state_dict(state_dict)\n",
        "      style_model.to(device='cuda:0')\n",
        "  style_imgs = []\n",
        "\n",
        "  if CLASS_SPECIFIC:\n",
        "    # Detectron2 setup\n",
        "    cfg = get_cfg()\n",
        "    # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "    # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
        "    predictor = DefaultPredictor(cfg)\n",
        "\n",
        "    # get class list \n",
        "    img = cv2.imread(ORG_DIR + base_names[0])\n",
        "    v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "    class_list = v.metadata.thing_classes\n",
        "\n",
        "    # convert object classes to index\n",
        "    num_class_list = []\n",
        "    for ob in objects:\n",
        "      num_class_list.append(class_list.index(ob))\n",
        "\n",
        "  for base in tqdm(base_names):\n",
        "    img = cv2.imread(ORG_DIR + base)\n",
        "\n",
        "    if CLASS_SPECIFIC:\n",
        "      outputs = predictor(img)\n",
        "      binary_mask = create_mask(outputs, num_class_list, img)\n",
        "      cv2.imwrite(MASKS_DIR + base, binary_mask)\n",
        "\n",
        "    content_image = img\n",
        "    content_image = content_transform(content_image)\n",
        "    content_image = content_image.unsqueeze(0).to(device='cuda:0')\n",
        "    with torch.no_grad():\n",
        "      style_img = style_model(content_image)\n",
        "      style_img = torch.squeeze(style_img)\n",
        "      style_img = style_img.permute(1, 2, 0).detach().cpu().numpy()\n",
        "    cv2.imwrite(STYLE_DIR + base, style_img) \n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bqkdOpliOdV"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "def run_engine(base_names):\n",
        "  # stylization setup\n",
        "  content_transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Lambda(lambda x: x.mul(255))\n",
        "  ])\n",
        "  with torch.no_grad():\n",
        "      style_model = TransformerNet()\n",
        "      state_dict = torch.load(STYLE_PTH)\n",
        "      # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
        "      for k in list(state_dict.keys()):\n",
        "          if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
        "              del state_dict[k]\n",
        "      style_model.load_state_dict(state_dict)\n",
        "      style_model.to(device='cuda:0')\n",
        "  style_imgs = []\n",
        "\n",
        "  if CLASS_SPECIFIC:\n",
        "    # Detectron2 setup\n",
        "    cfg = get_cfg()\n",
        "    # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "    # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
        "    predictor = DefaultPredictor(cfg)\n",
        "\n",
        "    # get class list \n",
        "    img = cv2.imread(ORG_DIR + base_names[0])\n",
        "    v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "    class_list = v.metadata.thing_classes\n",
        "    print(\"____CLASS LIST____\")\n",
        "    print(class_list)\n",
        "\n",
        "    # convert object classes to index\n",
        "    num_class_list = []\n",
        "    for ob in objects:\n",
        "      num_class_list.append(class_list.index(ob))\n",
        "\n",
        "  for base in tqdm(base_names):\n",
        "    img = cv2.imread(ORG_DIR + base)\n",
        "\n",
        "    if CLASS_SPECIFIC:\n",
        "      outputs = predictor(img)\n",
        "      print(\"____PREDICTED OUTPUTS____\")\n",
        "      print(outputs)\n",
        "      binary_mask = create_mask(outputs, num_class_list, img)\n",
        "      print(\"____NUM CLASS LIST____\")\n",
        "      print(num_class_list)\n",
        "      # cv2_imshow(binary_mask)\n",
        "      cv2.imwrite(MASKS_DIR + base, binary_mask)\n",
        "\n",
        "    content_image = img\n",
        "    content_image = content_transform(content_image)\n",
        "    content_image = content_image.unsqueeze(0).to(device='cuda:0')\n",
        "    with torch.no_grad():\n",
        "      style_img = style_model(content_image)\n",
        "      style_img = torch.squeeze(style_img)\n",
        "      style_img = style_img.permute(1, 2, 0).detach().cpu().numpy()\n",
        "    cv2.imwrite(STYLE_DIR + base, style_img) \n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMEA71vjKzsE"
      },
      "source": [
        "**Change Backgound Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPcJdfkSAhqC"
      },
      "source": [
        "def change_background() : \n",
        "  original = cv2.imread(file_name)\n",
        "  background = cv2.imread('background.jpg')\n",
        "  bit_mask = cv2.imread('./temp_imgs/masks_imgs/' + file_name, 0)\n",
        "  final = cv2.imread('./temp_imgs/final_imgs/' + file_name)\n",
        "\n",
        "  height, width, channels = original.shape\n",
        "  resized_background = cv2.resize(background, (width, height))\n",
        "  equalizeImg = cv2.equalizeHist(bit_mask)\n",
        "\n",
        "  bitwise_and = cv2.bitwise_and(final, final, mask=equalizeImg)\n",
        "  bitwise_or = cv2.bitwise_or(resized_background, bitwise_and)\n",
        "\n",
        "  print(\"saving changed background to temp folder\")\n",
        "  CHANGE_BACKGROUND_DIR = \"temp_imgs/changed_background_imgs/\"\n",
        "  cv2.imwrite(CHANGE_BACKGROUND_DIR + \"changed_background_and.jpg\", bitwise_and)\n",
        "  cv2.imwrite(CHANGE_BACKGROUND_DIR + \"changed_background.jpg\", bitwise_or)\n",
        "  print(\"image background change completed\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW47yDC1ggkR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "outputId": "56f0df3a-bf6f-4374-8caf-5abb95ea6590"
      },
      "source": [
        "#@title Main\n",
        "import datetime\n",
        "! pip install pyheif\n",
        "import pyheif\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "is_video = None\n",
        "for key in files_list:\n",
        "  print(\"Stylizing \" + key)\n",
        "  create_temp()\n",
        "  kind = filetype.guess(key)\n",
        "  file_type = kind.mime.split(\"/\")[0]\n",
        "  if file_type == \"video\":\n",
        "    is_video = True\n",
        "  else:\n",
        "    is_video = False\n",
        "  file_name = key\n",
        "\n",
        "  \n",
        "\n",
        "  save_name = create_name(key)\n",
        "\n",
        "  # if video extract frames, else move image into ORG_DIR\n",
        "  \n",
        "  if is_video:\n",
        "    print(\"Extracting Frames...\")\n",
        "    base_names, vidcap = extract_frames(file_name)\n",
        "  else:\n",
        "    if file_name.split(\".\")[1].lower() == 'heic':\n",
        "      heif_file = pyheif.read(file_name)\n",
        "      image = Image.frombytes(\n",
        "          heif_file.mode, \n",
        "          heif_file.size, \n",
        "          heif_file.data,\n",
        "          \"raw\",\n",
        "          heif_file.mode,\n",
        "          heif_file.stride,\n",
        "          )\n",
        "      open_cv_image = np.array(image) \n",
        "      # Convert RGB to BGR \n",
        "      open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
        "      cv2.imwrite(ORG_DIR + file_name.split(\".\")[0] + \".png\", open_cv_image)\n",
        "      base_names = [file_name.split(\".\")[0] + \".png\"]\n",
        "    else:\n",
        "      img = cv2.imread(file_name)\n",
        "      cv2.imwrite(ORG_DIR + file_name, img)\n",
        "      base_names = [file_name]\n",
        "  print(\"Running Engine...\")\n",
        "  run_engine(base_names)\n",
        "  print(\"Finalizing images...\")\n",
        "  final_imgs(base_names)\n",
        "  if is_video:\n",
        "    print(\"Generating video...\")\n",
        "    create_vid(base_names, vidcap, save_name)\n",
        "  else:\n",
        "    img = cv2.imread(FINAL_DIR + base_names[0])\n",
        "    cv2.imwrite(FINISHED_PRODUCTS + save_name + \".png\", img)\n",
        "  # remove_temp()\n",
        "  # command = \"rm \" + file_name\n",
        "  # subprocess.call(command, shell=True)\n",
        "\n",
        "zip_file = media_folder + \".zip\"\n",
        "\n",
        "# print(\"zipping media...\")\n",
        "# command = \"zip -r \"+ zip_file + \" \" + FINISHED_PRODUCTS\n",
        "# subprocess.call(command, shell=True)\n",
        "\n",
        "# print(\"downloading media files...\")\n",
        "# files.download(zip_file)\n",
        "\n",
        "change_background()\n",
        "\n",
        "zip_file_temp = \"./temp_imgs\" + \".zip\"\n",
        "\n",
        "print(\"zipping temp...\")\n",
        "command = \"zip -r \"+ zip_file_temp + \" \" + TEMP_DIR\n",
        "subprocess.call(command, shell=True)\n",
        "\n",
        "print(\"downloading media files...\")\n",
        "files.download(zip_file_temp)\n",
        "\n",
        "command = \"rm -r \" + FINISHED_PRODUCTS\n",
        "subprocess.call(command, shell=True)\n",
        "\n",
        "print(\"Finished!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyheif in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pyheif) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->pyheif) (2.20)\n",
            "Stylizing background.jpg\n",
            "Running Engine...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____CLASS LIST____\n",
            "['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 44.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____PREDICTED OUTPUTS____\n",
            "{'instances': Instances(num_instances=0, image_height=417, image_width=626, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64), pred_masks: tensor([], device='cuda:0', size=(0, 417, 626), dtype=torch.bool)])}\n",
            "____NUM CLASS LIST____\n",
            "[2]\n",
            "Finalizing images...\n",
            "Stylizing car race.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running Engine...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____CLASS LIST____\n",
            "['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 44.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "____PREDICTED OUTPUTS____\n",
            "{'instances': Instances(num_instances=2, image_height=427, image_width=640, fields=[pred_boxes: Boxes(tensor([[286.9599, 184.4391, 622.8989, 376.5223],\n",
            "        [ 19.9772,  85.3878, 339.3846, 227.7249]], device='cuda:0')), scores: tensor([0.9930, 0.9807], device='cuda:0'), pred_classes: tensor([2, 2], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]],\n",
            "\n",
            "        [[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
            "____NUM CLASS LIST____\n",
            "[2]\n",
            "Finalizing images...\n",
            "saving changed background to temp folder\n",
            "image background change completed\n",
            "zipping temp...\n",
            "downloading media files...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ccb55972-c3ac-4061-84f5-7ddafb2697b1\", \"temp_imgs.zip\", 954687)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}